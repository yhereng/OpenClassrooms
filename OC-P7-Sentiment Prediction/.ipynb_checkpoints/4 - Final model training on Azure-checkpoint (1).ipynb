{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"black\"><font size=\"7\"><br>\n",
    "     Project 7 - Advanced Model\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Uncomment and run cells if you don't have nltk and keras 2.4.1 installed on your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 19.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nltk) (4.60.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.6.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.4.1\n",
      "  Downloading Keras-2.4.1-py2.py3-none-any.whl (169 kB)\n",
      "\u001b[K     |████████████████████████████████| 169 kB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from keras==2.4.1) (1.18.5)\n",
      "Requirement already satisfied: h5py in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from keras==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: tensorflow in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from keras==2.4.1) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from keras==2.4.1) (1.5.2)\n",
      "Requirement already satisfied: pyyaml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from keras==2.4.1) (5.4.1)\n",
      "Requirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from h5py->keras==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (0.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (2.1.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (1.37.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (0.8.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (3.15.8)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (0.35.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow->keras==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (1.29.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (50.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (4.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (4.2.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (3.10.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow->keras==2.4.1) (3.7.4.3)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.3.1\n",
      "    Uninstalling Keras-2.3.1:\n",
      "      Successfully uninstalled Keras-2.3.1\n",
      "Successfully installed keras-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1620909980985
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import math\n",
    "import nltk\n",
    "import azureml.core\n",
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620909985533
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1620909986824
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core import Workspace, Datastore, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1620910000723
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = 'your_subscription_id'\n",
    "resource_group = 'OC-P7'\n",
    "workspace_name = 'P7_ML'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='Data_train')\n",
    "data = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1620910000937
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1598400, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1620910003162
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(data, y = None):\n",
    "    '''tokenizes input dataframe considering words of 2 and more characters\n",
    "       and lowercase text and remove numbers\n",
    "    \n",
    "       Parameters\n",
    "       ----------\n",
    "       data : Pandas series to tokenize\n",
    "       \n",
    "       Returns\n",
    "       --------\n",
    "       Pandas series list of tokens'''\n",
    "              \n",
    "        \n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w{2,}')\n",
    "    sentences = data.str.lower()\n",
    "    sentences = sentences.str.replace('\\d+', '',regex=True)\n",
    "    results = sentences.apply(tokenizer.tokenize)\n",
    "  \n",
    "    return results\n",
    "\n",
    "#Create a transformer for pipeline integration\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "tokenizer_transformer = FunctionTransformer(func=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing text data to fit Keras requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the NN needs an input matrix with documents represented as an interger list, each interger is a word. we'll choose sequence length to be tweet_length(based on EDA it makes sense, if less than tweet_length use 0 padding) and vocabulary size max_token (based on preliminary EDA). We'll use Keras' vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1620910007700
    }
   },
   "outputs": [],
   "source": [
    "tweet_length = 30\n",
    "max_tokens = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1620910008559
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1620910010580
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data['text']\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15,random_state=33,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1620910012338
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1358640,) \n",
      " y_train shape: (1358640,) \n",
      " X_test shape : (239760,) \n",
      " y_test shape: (239760,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape} \\n y_train shape: {y_train.shape} \\n X_test shape : {X_test.shape} \\n y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1620910013979
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_sets(X_train,X_test,vocab_length=20000,tweet_length=30):\n",
    "    '''Compute and return the vectors of the documents in X_train and X_test with a fixed length'''\n",
    "    vectorizer = TextVectorization(max_tokens=vocab_length,output_sequence_length=tweet_length)\n",
    "    vectorizer.adapt(X_train.values)\n",
    "    voc = vectorizer.get_vocabulary() # vocabulary for futur use\n",
    "    word_index = dict(zip(voc, range(len(voc)))) # word index for futur use\n",
    "    return (vectorizer(X_train.values.reshape((X_train.values.shape[0],1))),\n",
    "            vectorizer(X_test.values.reshape((X_test.values.shape[0],1))),\n",
    "            voc,\n",
    "            word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1620910053851
    }
   },
   "outputs": [],
   "source": [
    "X_train_vect, X_test_vect, voc, word_index = vectorize_sets(\n",
    "    tokenizer_transformer.transform(X_train).str.join(sep=' '),\n",
    "    tokenizer_transformer.transform(X_test).str.join(sep=' '),\n",
    "    vocab_length=max_tokens,\n",
    "    tweet_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Save vocabulary for futur use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620910054198
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vocabulary.pkl','wb') as f:\n",
    "    pickle.dump(voc,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620900132662
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "with open('vocabulary.pkl','rb') as f:\n",
    "    test_voc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the embedding matrix ( word / coeff matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Glove embedding shows better results, we'll use it from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1620910099270
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.twitter.27B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the embedding matrix which can be used in a Keras Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1620910099473
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 42241 words (17759 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "misses_word=[]\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        misses_word.append(word)\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1620910099632
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    input_length=tweet_length,\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Textvectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620910099805
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=60000,\n",
    "    output_sequence_length=30)\n",
    "vectorize_layer.set_vocabulary(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1620910099966
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620910100132
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "gather": {
     "logged": 1620913058050
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_1 (TextVe (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 30, 100)           6000200   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,117,577\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 6,000,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_name = 'LSTM - GLoVe embedding'\n",
    "\n",
    "lstm_out=128\n",
    "lr=0.01\n",
    "dp=0.2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorize_layer)\n",
    "model.add(embedding_layer)\n",
    "#model.add(LSTM(lstm_out,dropout=dp,return_sequences=True))\n",
    "model.add(LSTM(lstm_out,dropout=dp))\n",
    "#model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer=opt,metrics = ['accuracy','AUC'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1620914721659
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1327/1327 [==============================] - 324s 244ms/step - loss: 0.5140 - accuracy: 0.7391 - auc_1: 0.8228 - val_loss: 0.4385 - val_accuracy: 0.7946 - val_auc_1: 0.8787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "1327/1327 [==============================] - 326s 245ms/step - loss: 0.4514 - accuracy: 0.7856 - auc_1: 0.8692 - val_loss: 0.4244 - val_accuracy: 0.8028 - val_auc_1: 0.8861\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "1327/1327 [==============================] - 325s 245ms/step - loss: 0.4413 - accuracy: 0.7923 - auc_1: 0.8756 - val_loss: 0.4213 - val_accuracy: 0.8047 - val_auc_1: 0.8890\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "1327/1327 [==============================] - 327s 247ms/step - loss: 0.4363 - accuracy: 0.7953 - auc_1: 0.8786 - val_loss: 0.4167 - val_accuracy: 0.8075 - val_auc_1: 0.8907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "1327/1327 [==============================] - 334s 252ms/step - loss: 0.4339 - accuracy: 0.7965 - auc_1: 0.8801 - val_loss: 0.4167 - val_accuracy: 0.8075 - val_auc_1: 0.8915\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(tokenizer_transformer.transform(X_train).str.join(sep=' '),\n",
    "     y_train,\n",
    "      batch_size=1024,\n",
    "       epochs=5,\n",
    "        validation_data=(tokenizer_transformer.transform(X_test).str.join(sep=' '), y_test))\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620814812820
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620807074353
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620807132481
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Performance on the common dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620912680400
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '8781751c-70dd-441a-8c45-2274208851c0'\n",
    "resource_group = 'OC-P7'\n",
    "workspace_name = 'P7_ML'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='tweets_common')\n",
    "data = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620914738757
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data['pred']=model.predict(tokenizer_transformer.transform(data['text']).str.join(sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620914740312
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data['pred_bin']=[1 if data.loc[i,'pred']>=0.5 else 0 for i in data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620914741940
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.798125"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data['target'],data['pred_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620814900224
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1516037</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow, its later than I feel, better wrap up ano...</td>\n",
       "      <td>0.677863</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>589923</td>\n",
       "      <td>0</td>\n",
       "      <td>@lemonissimo I think the reason I twitted so m...</td>\n",
       "      <td>0.340094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>213819</td>\n",
       "      <td>0</td>\n",
       "      <td>@GericaQuinn ahhhhh! dude u suck! lmao. jk! bu...</td>\n",
       "      <td>0.722751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10047</td>\n",
       "      <td>0</td>\n",
       "      <td>...aaaand there goes that great day  RIP Mrs W...</td>\n",
       "      <td>0.221471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1330460</td>\n",
       "      <td>1</td>\n",
       "      <td>another morning joe free morning ahhhh ... sun...</td>\n",
       "      <td>0.913709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>363111</td>\n",
       "      <td>0</td>\n",
       "      <td>@cdouglasroberts.... awww  Im across the state...</td>\n",
       "      <td>0.111212</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>247946</td>\n",
       "      <td>0</td>\n",
       "      <td>My mum doesn't allow me to listen to Radio:Act...</td>\n",
       "      <td>0.183729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1179315</td>\n",
       "      <td>1</td>\n",
       "      <td>awesome, i love the quality from my nikon came...</td>\n",
       "      <td>0.958652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1349427</td>\n",
       "      <td>1</td>\n",
       "      <td>i think @emmacade always looks hot no matter w...</td>\n",
       "      <td>0.611167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>296578</td>\n",
       "      <td>0</td>\n",
       "      <td>that's all i have to say</td>\n",
       "      <td>0.393861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column1  target                                               text  \\\n",
       "0  1516037       1  Wow, its later than I feel, better wrap up ano...   \n",
       "1   589923       0  @lemonissimo I think the reason I twitted so m...   \n",
       "2   213819       0  @GericaQuinn ahhhhh! dude u suck! lmao. jk! bu...   \n",
       "3    10047       0  ...aaaand there goes that great day  RIP Mrs W...   \n",
       "4  1330460       1  another morning joe free morning ahhhh ... sun...   \n",
       "5   363111       0  @cdouglasroberts.... awww  Im across the state...   \n",
       "6   247946       0  My mum doesn't allow me to listen to Radio:Act...   \n",
       "7  1179315       1  awesome, i love the quality from my nikon came...   \n",
       "8  1349427       1  i think @emmacade always looks hot no matter w...   \n",
       "9   296578       0                           that's all i have to say   \n",
       "\n",
       "       pred  pred_bin  \n",
       "0  0.677863         1  \n",
       "1  0.340094         0  \n",
       "2  0.722751         1  \n",
       "3  0.221471         0  \n",
       "4  0.913709         1  \n",
       "5  0.111212         0  \n",
       "6  0.183729         0  \n",
       "7  0.958652         1  \n",
       "8  0.611167         1  \n",
       "9  0.393861         0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Registering the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620741799147
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model LSTM_Glove\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model.register(workspace, model_name='LSTM_Glove', model_path=os.getcwd()+'/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.8.1\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - keras<=2.4.3\n",
    "  - nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b9ca7990e127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": [
    "tensorflow.__version__"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "notify_time": "10",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "408.767px",
    "left": "1625px",
    "right": "20px",
    "top": "120px",
    "width": "275px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
